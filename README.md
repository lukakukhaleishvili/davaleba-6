# davaleba-6


# ყურადღების (Attention) შრის დემო — Transformer-ის ახსნით

ეს პროექტი შექმნილია იმისთვის, რომ მკითხველს მარტივად, ვიზუალურად და ეტაპობრივად აუხსნას როგორ მუშაობს ყურადღების მექანიზმი (Attention mechanism) ტრანსფორმერ ქსელებში (Transformer networks).

---

## ძირითადი მოდულები

ფაილი შეიცავს შემდეგ ნაწილებს:

---

### 1. `explanation()`: რა არის Attention?

ფუნქცია ბეჭდავს ტექსტურ ახსნას Attention-ის მექანიზმზე:

- **Query (Q)** – ის, რაც ცდილობს იპოვოს შესაბამისი ელემენტები
- **Key (K)** – იმ ელემენტების წარმომადგენლობა, რომლებსაც ვადარებთ
- **Value (V)** – ინფორმაცია, რომელიც უნდა მივიღოთ შესაბამისობის მიხედვით

გამოიყენება ფორმულა:

Attention(Q, K, V) = softmax(QKᵀ / √dₖ) · V


---

### 2. `demonstrate_matrix_multiplication()`: მატრიცული ოპერაციის მაგალითი

ეს ფუნქცია გიჩვენებთ მარტივ მაგალითს:

- როგორ გამოიყურება Q, K, V როგორც მატრიცები
- როგორ ხდება Q @ Kᵀ-ს გამოთვლა
- როგორ მუშაობს `softmax`
- როგორ ითვლება საბოლოო attention output

გამოგადგებათ მათემატიკური წარმოდგენის გასამყარებლად.

---

### 3. `attention_heatmap()`: ვიზუალიზაცია

ქმნის სითბოს რუკას (`heatmap`) — როგორ არის გადანაწილებული ყურადღება ტოკენებს შორის.

მაგალითად:
ტოკენები: ['მე', 'ვწერ', 'კოდს']


შეიძლება დაგვანახოს რომ „კოდს“ ყველაზე მეტად უკავშირდება „ვწერ“.

გამოიყენება `matplotlib` და `seaborn`.

---

### 4. `explain_multihead()`: მრავალთავიანი ყურადღება (Multi-head)

მრავალთავიანი ყურადღება გულისხმობს:

- რამდენიმე დამოუკიდებელი ყურადღების „თავს“
- თითოეული თავი ამუშავებს განსხვავებულ ასპექტს
- საბოლოოდ ეს თავები ერთიანდება და ქმნის უფრო მდიდარ წარმოდგენას
 გამოიყენება რეალურ Transformer-ებში, მაგალითად BERT, GPT და სხვ.

---

### 5. `explain_masked()`: ნიღბიანი ყურადღება (Masked Attention)

გამოიყენება **დეკოდერის** ნაწილში:

- მიზანია მოდელმა არ დაინახოს მომავალი ტოკენები წინასწარ
- ნიღბავს ყურადღების ქულებს მარჯვენა მიმართულებით (triangular mask)

 აუცილებელია თარგმნის ან ტექსტის გენერაციისას სწორი თანმიმდევრობის უზრუნველსაყოფად.

---

## გაშვება

ფაილი შეგიძლიათ გაუშვათ პირდაპირ:

```bash
python attention_layer_doc.py
